{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10062268027795d9fc6b67e7d14831bcd2a637bf"
   },
   "source": [
    "**1. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8e6487a2d9e308a537aaf98b0d5f8c9601422c4"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "                    \n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras import Model\n",
    "from keras.preprocessing import sequence,text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "stemmer=SnowballStemmer('english')\n",
    "lemma=WordNetLemmatizer()\n",
    "from string import punctuation\n",
    "\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c1d087cbbb14903034ef3c55ac3ccdad2421c16"
   },
   "outputs": [],
   "source": [
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9de21b87c41652b893a4e3a689e18353ed6c198"
   },
   "outputs": [],
   "source": [
    "#Import Data\n",
    "df = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2beb55c26b70ee68335b70991153bc74ac319415"
   },
   "source": [
    "**2. Data Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7cb6be67360ce82e11acbea2b985379449f0d08a"
   },
   "outputs": [],
   "source": [
    "print(\"Columns :\", df.columns)                        #printing column names\n",
    "print(\"Row 0 :\") \n",
    "print(\"qid :\", df.iloc[0]['qid'])                     #first example qid\n",
    "print(\"question_text : \", df.iloc[0]['question_text'])#first example text\n",
    "print(\"label :\", df.iloc[0]['target'])                #first example label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "37f47c0a45007b32051ab94604c4ecba32b8bf97",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Exploring insincere questions\n",
    "print(\"First 10 insincere questions:\\n\")\n",
    "insincere_question = df[df['target'] == 1]['question_text'].values\n",
    "for i in range(10):\n",
    "    print(insincere_question[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6d7d44e4858b4b4d2058e1f9c0dbc5dc9466863"
   },
   "source": [
    "**3. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d3db666cd39fd4eaf7f0800c3fe2c482627575bf"
   },
   "outputs": [],
   "source": [
    "def clean_review(review_col):\n",
    "    review_corpus=[]\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    for i in range(0,len(review_col)):\n",
    "        review=str(review_col[i])\n",
    "        review=re.sub('[^a-zA-Z]',' ',review)\n",
    "        word_token = word_tokenize(str(review).lower())\n",
    "        review=[lemma.lemmatize(w) for w in word_token if w not in stops]\n",
    "        review=' '.join(review)\n",
    "        review_corpus.append(review)\n",
    "    return review_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "106f226d3ef99d7d0d094f1b1865f8c1ff8aa4fb"
   },
   "outputs": [],
   "source": [
    "df['clean_question']=clean_review(df['question_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "875f568ddcf8b2898340d5eb9b0b1a8ab68e375d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['clean_question'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2f59e238b50dcd6568a344b1211562146a76416"
   },
   "outputs": [],
   "source": [
    "y_train = df['target'].values\n",
    "X_train_text = df['clean_question'].values\n",
    "\n",
    "#Split into training (70%), validation (15%) and test (15%) data \n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(X_train_text, y_train, test_size=0.3)\n",
    "X_val_text, X_test_text,y_val,y_test = train_test_split(X_val_text, y_val, test_size=0.5)\n",
    "\n",
    "#Parameters to preprocess text data\n",
    "num_unique_word = 166289 \n",
    "MAX_QUESTION_LEN=125 #max allowable words in a question\n",
    "MAX_FEATURES = num_unique_word #ceil on the number of unique words from courpus to use\n",
    "MAX_WORDS = MAX_QUESTION_LEN #max allowable words in a question\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES) #tokenize training data\n",
    "tokenizer.fit_on_texts(list(X_train_text))\n",
    "X_train = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_val = tokenizer.texts_to_sequences(X_val_text)\n",
    "X_test = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_WORDS)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=MAX_WORDS)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=MAX_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "678ed8c42c91223f0c8c31173a00d10ea8e6b9ab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Exmaple text: {0}\\n\\n\".format(X_val_text[1]))\n",
    "print(\"Corresponding vector\\n: {0}\\n\\n\".format(X_val[1]))\n",
    "print(\"The word 'best' corresponds to token # {0}\\n\".format(tokenizer.word_index.get('best')))\n",
    "print(\"The word 'home' corresponds to token # {0}\\n\".format(tokenizer.word_index.get('home')))\n",
    "print(\"The word 'remedy' corresponds to token # {0}\\n\".format(tokenizer.word_index.get('remedy')))\n",
    "print(\"The word 'migraine' corresponds to token # {0}\\n\".format(tokenizer.word_index.get('migraine')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d815caaeaa281f5bea4f3177aad9b8bca1d9cee8"
   },
   "source": [
    "**4. Basline Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3905c18cdfbce1e653747a081929ec07d12b7dea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, grid_search\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy.random import random_sample\n",
    "from matplotlib.pyplot import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dec168bd25134577947a9173d0ea76b27784cf0d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import random_sample\n",
    "from collections import Counter\n",
    "\n",
    "class BaselineModel:\n",
    "    def __init__(self,type='majority'):\n",
    "        self.type = type\n",
    "        print(\"Instance of {0} classifier\".format(self.type))\n",
    "    def fit(self, X , y):\n",
    "        if self.type=='majority':\n",
    "            counts = Counter(y)\n",
    "            freq_max = 0\n",
    "            for label, freq in counts.items():\n",
    "                if freq>freq_max:\n",
    "                    freq_max = freq\n",
    "                    self.label = label\n",
    "        elif self.type=='random':\n",
    "                self.label = 0\n",
    "                \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0],1))\n",
    "        if self.type == 'majority':\n",
    "            predictions = np.ones((X.shape[0],1)) * self.label\n",
    "        elif self.type == 'random':\n",
    "            predictions = random_sample((X.shape[0],1))\n",
    "            predictions[predictions>=0.5]=1\n",
    "            predictions[predictions<0.5]=0\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf4836f254708bf7accf94bd314cae8a9c1259b9"
   },
   "source": [
    "**Majority Classifier**\n",
    "\n",
    "Always predict majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2684f3237396f10f5d6d3ca3849abc7c8496f54d"
   },
   "outputs": [],
   "source": [
    "# 0 (sincere questions) is the majority class\n",
    "modelMaj = BaselineModel(type='majority')\n",
    "modelMaj.fit(X_train, y_train)\n",
    "predictions_val  = modelMaj.predict(X_val)\n",
    "predictions_test = modelMaj.predict(X_test)\n",
    "print(\"Majority Classifier,   Val acc: {0},   Val F1 Score: {1}\".format(metrics.accuracy_score(predictions_val, y_val),metrics.f1_score(predictions_val, y_val)))\n",
    "print(\"Majority Classifier,   Test acc: {0},   Test F1 Score: {1}\".format(metrics.accuracy_score(predictions_test, y_test),metrics.f1_score(predictions_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "77e2c03119e3f0958ba46e72260401b4cf6ce268"
   },
   "outputs": [],
   "source": [
    "predictions_val  = modelMaj.predict(X_val)\n",
    "predictions_test = modelMaj.predict(X_test)\n",
    "f1_val    , threshold_val   = line_search_f1_score(predictions_val , y_val)\n",
    "acc_val   , threshold_val   = line_search_acc_score(predictions_val, y_val)\n",
    "f1_test   , threshold_test  = line_search_f1_score(predictions_test , y_test)\n",
    "acc_test  , threshold_test  = line_search_acc_score(predictions_test, y_test)\n",
    "print(\"Majority Classifier,   Val_acc:  {0},   F1 Score: {1}\".format(acc_val,f1_val))\n",
    "print(\"Majority Classifier,   Test_acc: {0},   F1 Score: {1}\".format(acc_test,f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc4a4f63ea08d0bd74ef473ea2400e8ff303b9ef"
   },
   "source": [
    "**Random Classifier**\n",
    "\n",
    "Predict 0 or 1 randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "867e9f3d94d4340846f6ccad0bbe256b7c1f32c9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 0 (sincere questions) is the majority class\n",
    "modelRand = BaselineModel(type='random')\n",
    "modelRand.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0c84b1335fc8aac3024ec88cb12d03054ce4165"
   },
   "outputs": [],
   "source": [
    "predictions_val = modelRand.predict(X_val)\n",
    "predictions_test = modelRand.predict(X_test)\n",
    "f1_val    , threshold_val  = line_search_f1_score(predictions_val , y_val)\n",
    "acc_val   , threshold_val  = line_search_acc_score(predictions_val, y_val)\n",
    "f1_test   , threshold_test  = line_search_f1_score(predictions_test , y_test)\n",
    "acc_test  , threshold_test  = line_search_acc_score(predictions_test, y_test)\n",
    "print(\"Random Classifier,   Val_acc:  {0},   F1 Score: {1}\".format(acc_val,f1_val))\n",
    "print(\"Random Classifier,   Test_acc: {0},   F1 Score: {1}\".format(acc_test,f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "855d47260f1e7d744c6503dba59cfc783e8f46b3"
   },
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bce8afeed54ab02f2124cbd25e1fb5759ce48084"
   },
   "outputs": [],
   "source": [
    "modelLR  = LogisticRegression(random_state=0, solver='lbfgs',class_weight=\"balanced\",C=0.1)\n",
    "modelLR.fit(X_train, y_train)\n",
    "predictions_val = modelLR.predict(X_val)\n",
    "predictions_test = modelLR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "74314143d4591a716bea5d13ad9e469d479c8536"
   },
   "outputs": [],
   "source": [
    "predictions_val = modelLR.predict(X_val)\n",
    "predictions_test = modelLR.predict(X_test)\n",
    "f1_val    , threshold_val  = line_search_f1_score(predictions_val , y_val)\n",
    "acc_val   , threshold_val  = line_search_acc_score(predictions_val, y_val)\n",
    "f1_test   , threshold_test  = line_search_f1_score(predictions_test , y_test)\n",
    "acc_test  , threshold_test  = line_search_acc_score(predictions_test, y_test)\n",
    "print(\"Logistic Regression,   Val_acc:  {0},   F1 Score: {1}\".format(acc_val,f1_val))\n",
    "print(\"Logistic Regression,   Test_acc: {0},   F1 Score: {1}\".format(acc_test,f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc103edfd701c10d8247a4b80d26f57b7ef75220"
   },
   "source": [
    "**Random Forest** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dc9bb2549cd1541f899e3bbffa761801e977ff0"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelRF = RandomForestClassifier(criterion='gini', max_depth=10, class_weight='balanced')\n",
    "modelRF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd44a70125080ec1c6acce614e324d06d8e43825"
   },
   "outputs": [],
   "source": [
    "predictions_val = modelRF.predict(X_val)\n",
    "predictions_test = modelRF.predict(X_test)\n",
    "f1_val    , threshold_val   = line_search_f1_score(predictions_val , y_val)\n",
    "acc_val   , threshold_val   = line_search_acc_score(predictions_val, y_val)\n",
    "f1_test   , threshold_test  = line_search_f1_score(predictions_test , y_test)\n",
    "acc_test  , threshold_test  = line_search_acc_score(predictions_test, y_test)\n",
    "print(\"Random Forest,   Val_acc:  {0},   F1 Score: {1}\".format(acc_val,f1_val))\n",
    "print(\"Random Forest,   Test_acc: {0},   F1 Score: {1}\".format(acc_test,f1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7a256467f189db0854700f21ec5a492b40bf8f5"
   },
   "outputs": [],
   "source": [
    "#Functions to prepare word embeddings\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "def get_embed_mat(EMBEDDING_FILE, max_features,embed_dim):\n",
    "    # word vectors\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf8'))\n",
    "    # embedding matrix\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_features, len(word_index) + 1)\n",
    "    all_embs = np.stack(embeddings_index.values()) #for random init\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embed_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:       #use only the top 125 words as features\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    max_features = embedding_matrix.shape[0]\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "#gloveEmbed is the embedding matrix contatining a 300-dimensional vector for each of the 137803 unqiue words in\n",
    "#our vocabulary\n",
    "gloveEmbed = get_embed_mat('../input/embeddings/glove.840B.300d/glove.840B.300d.txt', MAX_FEATURES, 300)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f7369b1c9c63982f1de1c0634a0ef634a3e2ce1"
   },
   "outputs": [],
   "source": [
    "def line_search_f1_score(y_score, y_test):\n",
    "    max_f1_score = 0\n",
    "    opt_threshold = 0\n",
    "    for threshold in [i*0.01 for i in range(100)]:\n",
    "        y_preds = y_score > threshold\n",
    "        score = f1_score(y_preds, y_test)\n",
    "        if max_f1_score < score:\n",
    "            max_f1_score = score\n",
    "            opt_threshold = threshold\n",
    "    return max_f1_score, opt_threshold\n",
    "\n",
    "def line_search_acc_score(y_score, y_test):\n",
    "    max_acc_score = 0\n",
    "    opt_threshold = 0\n",
    "    for threshold in [i*0.01 for i in range(100)]:\n",
    "        y_preds = y_score > threshold\n",
    "        score = accuracy_score(y_preds, y_test)\n",
    "        if max_acc_score < score:\n",
    "            max_acc_score = score\n",
    "            opt_threshold = threshold\n",
    "    return max_acc_score, opt_threshold\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def __init__(self):\n",
    "        self.best_threshold = 0.5\n",
    "        self.best_f1_score = 0\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "        self.best_f1_score = 0\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "         idx = np.random.randint(0,self.validation_data[0].shape[0],1000)\n",
    "         val_predict = (np.asarray(self.model.predict(self.validation_data[0][idx], verbose=1))).round()\n",
    "         val_targ = self.validation_data[1][idx]\n",
    "         #_val_f1 = f1_score(val_targ, val_predict)\n",
    "         _val_f1, threshold = line_search_f1_score(val_targ, val_predict)\n",
    "         if _val_f1 > self.best_f1_score:\n",
    "                self.best_f1_score = _val_f1\n",
    "         self.best_threshold = threshold\n",
    "         _val_recall = recall_score(val_targ, val_predict)\n",
    "         _val_precision = precision_score(val_targ, val_predict)\n",
    "         self.val_f1s.append(_val_f1)\n",
    "         self.val_recalls.append(_val_recall)\n",
    "         self.val_precisions.append(_val_precision)\n",
    "         print(\" — val_f1: %f — val_precision: %f — val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "         return\n",
    " \n",
    "metric = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1f8682d14eeb8f1d426438f53ba638d77d8b7b98"
   },
   "source": [
    "**LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0363847eee062a70ce500e3768f0e078342ed54"
   },
   "outputs": [],
   "source": [
    "lstm_out = 200\n",
    "modelLSTM = Sequential()\n",
    "embedding_layer = Embedding(len(word_index) + 1,300,weights=[gloveEmbed],input_length=MAX_WORDS,trainable=False)\n",
    "modelLSTM.add(embedding_layer)\n",
    "modelLSTM.add(LSTM(lstm_out, dropout_U = 0.4, dropout_W = 0.4))\n",
    "modelLSTM.add(Dense(1,activation='sigmoid'))\n",
    "modelLSTM.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(modelLSTM.summary())\n",
    "modelLSTM.fit(X_train, y_train,epochs=2, batch_size=1024, verbose=1,callbacks=[metric], validation_data = (X_val,y_val),class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2def2eee19a73dc6fc6bcaeacc7b2f6bd5fdfa65",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions_val_lstm = modelLSTM.predict(X_val)\n",
    "predictions_test_lstm = modelLSTM.predict(X_test)\n",
    "\n",
    "f1_val   , threshold_val  = line_search_f1_score(predictions_val_lstm , y_val)\n",
    "acc_val  , threshold_val  = line_search_acc_score(predictions_val_lstm , y_val)\n",
    "f1_test  , threshold_test = line_search_f1_score(predictions_test_lstm, y_test)\n",
    "acc_test , threshold_test = line_search_acc_score(predictions_test_lstm, y_test)\n",
    "\n",
    "print(\"LSTM,   Val_acc: {0},   F1 Score: {1}\".format(acc_val, f1_val))\n",
    "print(\"LSTM,   Test acc: {0},   Test F1 Score: {1}\".format(acc_test,f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a32116ee8bcc811ac50ab561a8797b0e0689e8ce"
   },
   "source": [
    "**5. Dealing With Imbalanced Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73d834b4fce9b0ac5bb2607ab3068a73d51f91bf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "ennus = RandomUnderSampler(random_state=0, sampling_strategy='majority')\n",
    "X_train_bal, y_train_bal = ennus.fit_resample(X_train, y_train)\n",
    "X_val_bal, y_val_bal = ennus.fit_resample(X_val, y_val)\n",
    "X_test_bal, y_test_bal  = ennus.fit_resample(X_test , y_test)\n",
    "\n",
    "y_total    =np.concatenate((y_train,y_val,y_test), axis=0)\n",
    "y_total_bal=np.concatenate((y_train_bal,y_val_bal,y_test_bal), axis=0)\n",
    "n_rows = X_total.shape[0]\n",
    "n_rows_bal = X_total_bal.shape[0]\n",
    "n_insincere = len(y_total[y_total==1])\n",
    "n_insincere_bal = len(y_total_bal[y_total_bal==1])\n",
    "\n",
    "label_repart = pd.DataFrame(data={\"\" :[n_rows - n_insincere, n_insincere]}, index = [str(n_rows - n_insincere) + ' sincere questions', str(n_insincere) + ' insincere question'])\n",
    "label_repart.plot(kind='pie', title='Sincere-to-Insincere Ratio (Before Undersampling) ' + str(round(n_insincere / n_rows,2)*100) + \"%\", subplots=True, figsize=(8,8))\n",
    "label_repart = pd.DataFrame(data={\"\" :[n_rows_bal - n_insincere_bal, n_insincere_bal]}, index = [str(n_rows - n_insincere) + ' sincere questions', str(n_insincere) + ' insincere question'])\n",
    "label_repart.plot(kind='pie', title='Sincere-to-Insincere Ratio (After Undersampling) ' + str(round(n_insincere_bal / n_rows_bal,2)*100) + \"%\", subplots=True, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dca6eeb89b930581ba3d6adf1d0ca6b62117be44"
   },
   "source": [
    "**6. All Classifiers Using Balanced Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf4836f254708bf7accf94bd314cae8a9c1259b9"
   },
   "source": [
    "**Majority Classifier (Balanced Data)**\n",
    "\n",
    "Always predict majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2684f3237396f10f5d6d3ca3849abc7c8496f54d"
   },
   "outputs": [],
   "source": [
    "# 0 (sincere questions) is the majority class\n",
    "modelMaj_bal = BaselineModel(type='majority')\n",
    "modelMaj_bal.fit(X_train_bal, y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d682724a5862557310fc695e617ddf239aa4d2c"
   },
   "outputs": [],
   "source": [
    "predictions_val  = modelMaj_bal.predict(X_val_bal)\n",
    "predictions_test = modelMaj_bal.predict(X_test_bal)\n",
    "f1_val    , threshold_val  = line_search_f1_score(predictions_val , y_val_bal)\n",
    "acc_val   , threshold_val  = line_search_acc_score(predictions_val, y_val_bal)\n",
    "f1_test   , threshold_test  = line_search_f1_score(predictions_test , y_test_bal)\n",
    "acc_test  , threshold_test  = line_search_acc_score(predictions_test, y_test_bal)\n",
    "print(\"Majority Classifier,   Val acc: {0},   Val  F1 Score: {1}\".format(acc_val,f1_val))\n",
    "print(\"Majority Classifier,   Test_acc: {0},  Test F1 Score: {1}\".format(acc_test,f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc4a4f63ea08d0bd74ef473ea2400e8ff303b9ef"
   },
   "source": [
    "**Random Classifier (Balanced)**\n",
    "\n",
    "Predict 0 or 1 randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "867e9f3d94d4340846f6ccad0bbe256b7c1f32c9"
   },
   "outputs": [],
   "source": [
    "# 0 (sincere questions) is the majority class\n",
    "modelRand_bal = BaselineModel(type='random')\n",
    "modelRand_bal.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "197a51a8f5e23a4158c93f3e414cc7d1e5869583"
   },
   "outputs": [],
   "source": [
    "predictions_val  = modelRand_bal.predict(X_val_bal)\n",
    "predictions_test = modelRand_bal.predict(X_test_bal)\n",
    "f1_val    , threshold_val  = line_search_f1_score(predictions_val , y_val_bal)\n",
    "acc_val   , threshold_val  = line_search_acc_score(predictions_val, y_val_bal)\n",
    "f1_test   , threshold_test  = line_search_f1_score(predictions_test , y_test_bal)\n",
    "acc_test  , threshold_test  = line_search_acc_score(predictions_test, y_test_bal)\n",
    "print(\"Random Classifier,   Val acc: {0},   Val  F1 Score: {1}\".format(acc_val,f1_val))\n",
    "print(\"Random Classifier,   Test_acc: {0},  Test F1 Score: {1}\".format(acc_test,f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a41f6e389d961b4512e30022c46a001733e0d59e"
   },
   "source": [
    "**Logistic Regression (Balanced)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b71ad15b1d901f4961c43095beda655b01b4d2e3"
   },
   "source": [
    "Logistic Regression Hyperparameter Tuning\n",
    "\n",
    "https://medium.com/@aneesha/svm-parameter-tuning-in-scikit-learn-using-gridsearchcv-2413c02125a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f429661402a85271d2de06829e080a06d2d05f9"
   },
   "outputs": [],
   "source": [
    "# Helper function for tuning LR hyperparameters\n",
    "def LR_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    param_grid = {'C': Cs}\n",
    "    grid_s = grid_search.GridSearchCV(LogisticRegression(random_state=0, solver='lbfgs',class_weight=\"balanced\"), param_grid, cv=nfolds)\n",
    "    grid_s.fit(X, y)\n",
    "    grid_s.best_params_\n",
    "    return grid_s.best_params_\n",
    "\n",
    "#Finding best hyperparameters\n",
    "best_params = LR_param_selection(X_train_bal, y_train_bal, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e6145ef8c30cc01c35577edfcc8760cbe95e4b9a"
   },
   "outputs": [],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a30fa6d4ff5f4c9eb03bc9f84ee656d87e5f8b3a"
   },
   "outputs": [],
   "source": [
    "#Using best parameters for final model\n",
    "modelLR_bal  = LogisticRegression(random_state=0, solver='lbfgs',class_weight=\"balanced\",C=0.001)\n",
    "modelLR_bal.fit(X_train_bal, y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ba6fabbd8ef55696ed2e8f5cf5cd80a4e15a441"
   },
   "outputs": [],
   "source": [
    "predictions_val  = modelLR_bal.predict(X_val_bal)\n",
    "predictions_test = modelLR_bal.predict(X_test_bal)\n",
    "f1_val    , threshold_val  = line_search_f1_score(predictions_val , y_val_bal)\n",
    "acc_val   , threshold_val  = line_search_acc_score(predictions_val, y_val_bal)\n",
    "f1_test   , threshold_test  = line_search_f1_score(predictions_test , y_test_bal)\n",
    "acc_test  , threshold_test  = line_search_acc_score(predictions_test, y_test_bal)\n",
    "print(\"Logistic Regression,   Val acc: {0},   Val  F1 Score: {1}\".format(acc_val,f1_val))\n",
    "print(\"Logistic Regression,   Test_acc: {0},  Test F1 Score: {1}\".format(acc_test,f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef12bd4431ade6d65693cd84553b23652ee07630"
   },
   "source": [
    "**Random Forest (Balanced)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88cebdde9f834555d918a695cdb1c5cf34c719ca"
   },
   "source": [
    "Random Forest Hyperparameter Tuning\n",
    "\n",
    "https://medium.com/@aneesha/svm-parameter-tuning-in-scikit-learn-using-gridsearchcv-2413c02125a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "177b0c6e4e4c79116d165a78f5850aa355e241c5"
   },
   "outputs": [],
   "source": [
    "# Helper function for tuning RF hyperparameters\n",
    "def RF_param_selection(X, y, nfolds):\n",
    "    depths = [1,2,4,8,16,32]\n",
    "    param_grid = {'max_depth': depths}\n",
    "    grid_s = grid_search.GridSearchCV(RandomForestClassifier(criterion='gini', max_depth=10, class_weight='balanced'), param_grid, cv=nfolds)\n",
    "    grid_s.fit(X, y)\n",
    "    grid_s.best_params_\n",
    "    return grid_s.best_params_\n",
    "\n",
    "#Finding best hyperparameters\n",
    "best_params = RF_param_selection(X_train_bal, y_train_bal, 5)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dc9bb2549cd1541f899e3bbffa761801e977ff0"
   },
   "outputs": [],
   "source": [
    "modelRF_bal = RandomForestClassifier(criterion='gini', max_depth=16, class_weight='balanced')\n",
    "modelRF_bal.fit(X_train_bal, y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd44a70125080ec1c6acce614e324d06d8e43825",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_val  = modelRF_bal.predict(X_val_bal)\n",
    "predictions_test = modelRF_bal.predict(X_test_bal)\n",
    "f1_val    , threshold_val  = line_search_f1_score(predictions_val , y_val_bal)\n",
    "acc_val   , threshold_val  = line_search_acc_score(predictions_val, y_val_bal)\n",
    "f1_test   , threshold_test  = line_search_f1_score(predictions_test , y_test_bal)\n",
    "acc_test  , threshold_test  = line_search_acc_score(predictions_test, y_test_bal)\n",
    "print(\"Random Forest,   Val acc:  {0},   Val  F1 Score: {1}\".format(acc_val,f1_val))\n",
    "print(\"Random Forest,   Test_acc: {0},   Test F1 Score: {1}\".format(acc_test,f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e76ffd24784896dd907971a29bd08138e56d1c4"
   },
   "source": [
    "**LSTM (Balanced)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cdff86a454d7d040db0c4145c5b49d8295e7d543"
   },
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2041269ea01c588d623bd0684badd7fc1bf04cab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropU=[0.2,0.4,0.6,0.8]\n",
    "dropW=[0.2,0.4,0.6,0.8]\n",
    "matrixF1  = np.zeros((4,4))\n",
    "matrixAcc = np.zeros((4,4))\n",
    "\n",
    "for i in range(len(dropU)):\n",
    "    for j in range(len(dropW)):\n",
    "        lstm_out = 200\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(LSTM(lstm_out, dropout_U = dropU[i], dropout_W = dropW[j]))\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "        print(model.summary())\n",
    "        model.fit(X_train_bal, y_train_bal,epochs=2, batch_size=1024, verbose=1,callbacks=[metric], validation_data = (X_val_bal,y_val_bal))\n",
    "        predictions_val = model.predict(X_val_bal)\n",
    "        f1_val   , threshold_val  = line_search_f1_score(predictions_val , y_val_bal)\n",
    "        acc_val  , threshold_val  = line_search_acc_score(predictions_val , y_val_bal)\n",
    "        matrixF1[i,j] = f1_val\n",
    "        matrixAcc[i,j] = acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6758ee51ec30db6b6c3ec52bdf613168d982bb13"
   },
   "outputs": [],
   "source": [
    "print(\"Matrix of F1  values:\\n\",matrixF1,\"\\n\")\n",
    "print(\"Matrix of Acc values:\\n\",matrixAcc,\"\\n\")\n",
    "dUf1, dWf1 = np.unravel_index(np.argmax(matrixF1, axis=None), matrixF1.shape)\n",
    "dUAc, dWAc = np.unravel_index(np.argmax(matrixAcc, axis=None), matrixAcc.shape)\n",
    "print(\"Max F1  : {0}  @ dropU= {1}, dropW= {2}\".format(np.max(matrixF1) , dropU[dUf1], dropW[dWf1]))\n",
    "print(\"Max Acc : {0}  @ dropU= {1}, dropW= {2}\".format(np.max(matrixAcc), dropU[dUAc] , dropW[dWAc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27b63c45c849f33878767f0efbddeb1afa4d5a99",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training LSTM with best dropout rates dropU=0.2, dropW=0.2\n",
    "lstm_out = 200\n",
    "modelLSTM_bal = Sequential()\n",
    "modelLSTM_bal.add(embedding_layer)\n",
    "modelLSTM_bal.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "modelLSTM_bal.add(Dense(1,activation='sigmoid'))\n",
    "modelLSTM_bal.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "modelLSTM_bal.fit(X_train_bal, y_train_bal,epochs=2, batch_size=1024, verbose=1,callbacks=[metric], validation_data = (X_val_bal,y_val_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32a9a4d8a73722450329ae9392fac7e01f0cba90"
   },
   "outputs": [],
   "source": [
    "predictions_val = modelLSTM_bal.predict(X_val_bal)\n",
    "predictions_test = modelLSTM_bal.predict(X_test_bal)\n",
    "f1_val   , threshold_val  = line_search_f1_score(predictions_val , y_val_bal)\n",
    "acc_val  , threshold_val  = line_search_acc_score(predictions_val , y_val_bal)\n",
    "f1_test  , threshold_test = line_search_f1_score(predictions_test, y_test_bal)\n",
    "acc_test , threshold_test = line_search_acc_score(predictions_test, y_test_bal)\n",
    "\n",
    "print(\"LSTM,   Val_acc : {0},    F1 Score: {1}\".format(acc_val, f1_val))\n",
    "print(\"LSTM,   Test acc: {0},   Test F1 Score: {1}\".format(acc_test,f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c02a3d8cf14abcd43952ee0cf336dd77b5eabd08"
   },
   "source": [
    "**Saving all Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef2e4430d6b9a8a1928eef8b56d727e755709626"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'modelMaj_bal'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(modelMaj_bal,outfile)\n",
    "outfile.close()\n",
    "filename = 'modelRand_bal'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(modelRand_bal,outfile)\n",
    "outfile.close()\n",
    "filename = 'modelLR_bal'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(modelLR_bal,outfile)\n",
    "outfile.close()\n",
    "filename = 'modelRF_bal'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(modelRF_bal,outfile)\n",
    "outfile.close()\n",
    "filename = 'modelLSTM_bal'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(modelLSTM_bal,outfile)\n",
    "outfile.close()\n",
    "filename = 'modelMaj_bal'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(modelMaj_bal,outfile)\n",
    "outfile.close()\n",
    "filename = 'modelRand_bal'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(modelRand_bal,outfile)\n",
    "outfile.close()\n",
    "filename = 'modelLR_bal'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(modelLR_bal,outfile)\n",
    "outfile.close()\n",
    "filename = 'modelRF_bal'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(modelRF_bal,outfile)\n",
    "outfile.close()\n",
    "filename = 'modelLSTM_bal'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(modelLSTM_bal,outfile)\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
